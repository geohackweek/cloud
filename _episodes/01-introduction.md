---
title: "Intro to cloud computing in geoscience"
teaching: 30
exercises: 0
questions:
- "What are the basic virtues / benefits of using the public cloud?"
- "What are cloud drawbacks?"
- "How would I go about getting started with cloud computing?"
objectives:
- Understand the data concept set { compute, store, manage, web, services } 
- Understand the research computing concept set { build, test, share }
- Discuss how cloud can overcome or bypass traditional research computing bottlenecks
keypoints:
- cloud is cost-effective via utility (pay as you go) model (three penny opera)
- cloud is secure via encryption and identity/access management tools
- cloud has a cost of entry: You have to learn the ropes
- cloud cost is dropping (e.g. consider Spot Market) so this may be a matter of not if but when
- cloud uses the term 'scale' and that translates to 'I am (generally) not compute limited anymore.' 
- cloud is also growing Services: Advanced concept, the metaphor is 'dinner without plates or table'.
---
### Overview:

Geoscientists often need to manipulate datasets under four goals: Perfunctory processing, exploratory analysis, 
store for later, share with colleagues. A simple example is 'I would like to pull data from the ARGO database
and from satellite Sea Surface Temperature maps 
in order to understand the ocean temperature and pH variability in the Northeast Pacific over the past five years.' 
For subsetting and other procedural details you might refer to the 'multi-dimensional data' lesson portion of 
geohackweek; here we take it as a given that you managed to extract this data from its sources. 


<br>
<img src="../fig/redqueen.png" width = "600" border = "10">
<br>

> ## What does this have to do with basic cloud computing?
> Well the idea is to make a dry exegesis on cloud computing a little more interesting with a geospatial goal driving it.
{: .callout}

### Data concept set { compute, store, manage, web, services }

Prior to about 1990...

In the early 1990s...

Two notable products came out of this effort:
[netcdf](http://www.unidata.ucar.edu/software/netcdf/docs/), optimized
for climate data analysis, and [hdf](https://www.hdfgroup.org/), used for many applications including
distribution of remote sensing datasets. 

Witness the glory of [netcdf toolkits](http://www.unidata.ucar.edu/software/netcdf/software.html) 

### Common data handling methods:

spatial and temporal dimensions.

### Challenges:

Nonsense.
